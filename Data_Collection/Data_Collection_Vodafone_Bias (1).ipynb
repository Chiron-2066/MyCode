{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper for Google Shareholder's Letters (Chiron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T06:36:13.897443Z",
     "start_time": "2020-05-07T06:36:10.930446Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import GetOldTweets3 as got\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:42:34.254000Z",
     "start_time": "2020-05-06T20:41:02.801712Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a list with the text on the page\n",
    "years = [\"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\"]\n",
    "\n",
    "# Creating an empty list to store letters' text\n",
    "google_letters = []\n",
    "\n",
    "# Scraping the founders' letters text on the \n",
    "# Alphabet's website\n",
    "for year in years:\n",
    "    driver = webdriver.Chrome(r'C:\\Users\\Chiron\\Desktop\\ARP\\chromedriver')\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.get(r'https://abc.xyz/investor/previous/#founders-letter')\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.find_element_by_link_text(year).click()\n",
    "    driver.implicitly_wait(10)\n",
    "    data = driver.find_elements_by_xpath(\"/html/body/main/div/div/section/div[@class='content']\")\n",
    "    driver.implicitly_wait(10)\n",
    "    \n",
    "    print(year)\n",
    "    \n",
    "    for item in data:\n",
    "        text = item.text\n",
    "        google_letters.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:42:45.739429Z",
     "start_time": "2020-05-06T20:42:45.725434Z"
    }
   },
   "outputs": [],
   "source": [
    "# Storing the specific letters by year\n",
    "letter_2009 = google_letters[0]\n",
    "letter_2010 = google_letters[1]\n",
    "letter_2011 = google_letters[2]\n",
    "letter_2012 = google_letters[3]\n",
    "letter_2013 = google_letters[4]\n",
    "letter_2014 = google_letters[5]\n",
    "letter_2015  =google_letters[6]\n",
    "letter_2016 = google_letters[7]\n",
    "letter_2017 = google_letters[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:42:47.680823Z",
     "start_time": "2020-05-06T20:42:47.667827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating df with the letters\n",
    "df = pd.DataFrame(np.column_stack([letter_2009,letter_2010,letter_2011,\n",
    "                  letter_2012,letter_2013,letter_2014,\n",
    "                  letter_2015,letter_2016,letter_2017]), \n",
    "                  columns = [\"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:42:49.573665Z",
     "start_time": "2020-05-06T20:42:49.559290Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the letters to csv \n",
    "df.to_csv('Google_letters.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Scraper (Mark and Elena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T06:36:26.433447Z",
     "start_time": "2020-05-07T06:36:26.417446Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating lists of keywords and companies to generate all possible combinations to search\n",
    "keywords = ['Data science', 'Data analytics', 'Digital transformation', 'Machine learning', 'Quantum computing', \n",
    "            'Big data', 'Artificial intelligence', 'Modelling', 'Internet of things', 'Distributed ledger', \n",
    "            'Blockchain', 'Natural Language Processing', 'Python', 'R', 'Java', 'Matlab', 'Telecom network']\n",
    "\n",
    "subject = ['Vodafone', 'O2', 'Three UK', 'EE', 'BT', 'Virgin Media', 'Telecom', 'Amazon', 'Google', 'Facebook', \n",
    "           'Microsoft']\n",
    "\n",
    "words_to_search = []\n",
    "\n",
    "for i in subject:\n",
    "    for j in keywords:\n",
    "        words_to_search.append(i + \" \" + j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T07:36:27.591217Z",
     "start_time": "2020-05-07T06:36:27.916440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vodafone Data science\n",
      "Vodafone Data analytics\n",
      "Vodafone Digital transformation\n",
      "Vodafone Machine learning\n",
      "Vodafone Quantum computing\n",
      "Vodafone Big data\n",
      "Vodafone Artificial intellignce\n",
      "Vodafone Modelling\n",
      "Vodafone Internet of things\n",
      "Vodafone Distributed ledger\n",
      "Vodafone Blockchain\n",
      "Vodafone Natural Language Processing\n",
      "Vodafone Python\n",
      "Vodafone R\n",
      "Vodafone Java\n",
      "Vodafone Matlab\n",
      "Vodafone Telecom network\n",
      "O2 Data science\n",
      "O2 Data analytics\n",
      "O2 Digital transformation\n",
      "O2 Machine learning\n",
      "O2 Quantum computing\n",
      "O2 Big data\n",
      "O2 Artificial intellignce\n",
      "O2 Modelling\n",
      "O2 Internet of things\n",
      "O2 Distributed ledger\n",
      "O2 Blockchain\n",
      "O2 Natural Language Processing\n",
      "O2 Python\n",
      "O2 R\n",
      "O2 Java\n",
      "O2 Matlab\n",
      "O2 Telecom network\n",
      "Three UK Data science\n",
      "Three UK Data analytics\n",
      "Three UK Digital transformation\n",
      "Three UK Machine learning\n",
      "Three UK Quantum computing\n",
      "Three UK Big data\n",
      "Three UK Artificial intellignce\n",
      "Three UK Modelling\n",
      "Three UK Internet of things\n",
      "Three UK Distributed ledger\n",
      "Three UK Blockchain\n",
      "Three UK Natural Language Processing\n",
      "Three UK Python\n",
      "Three UK R\n",
      "Three UK Java\n",
      "Three UK Matlab\n",
      "Three UK Telecom network\n",
      "EE Data science\n",
      "EE Data analytics\n",
      "EE Digital transformation\n",
      "EE Machine learning\n",
      "EE Quantum computing\n",
      "EE Big data\n",
      "EE Artificial intellignce\n",
      "EE Modelling\n",
      "EE Internet of things\n",
      "EE Distributed ledger\n",
      "EE Blockchain\n",
      "EE Natural Language Processing\n",
      "EE Python\n",
      "EE R\n",
      "EE Java\n",
      "EE Matlab\n",
      "EE Telecom network\n",
      "BT Data science\n",
      "BT Data analytics\n",
      "BT Digital transformation\n",
      "BT Machine learning\n",
      "BT Quantum computing\n",
      "BT Big data\n",
      "BT Artificial intellignce\n",
      "BT Modelling\n",
      "BT Internet of things\n",
      "BT Distributed ledger\n",
      "BT Blockchain\n",
      "BT Natural Language Processing\n",
      "BT Python\n",
      "BT R\n",
      "BT Java\n",
      "BT Matlab\n",
      "BT Telecom network\n",
      "Virgin Media Data science\n",
      "Virgin Media Data analytics\n",
      "Virgin Media Digital transformation\n",
      "Virgin Media Machine learning\n",
      "Virgin Media Quantum computing\n",
      "Virgin Media Big data\n",
      "Virgin Media Artificial intellignce\n",
      "Virgin Media Modelling\n",
      "Virgin Media Internet of things\n",
      "Virgin Media Distributed ledger\n",
      "Virgin Media Blockchain\n",
      "Virgin Media Natural Language Processing\n",
      "Virgin Media Python\n",
      "Virgin Media R\n",
      "Virgin Media Java\n",
      "Virgin Media Matlab\n",
      "Virgin Media Telecom network\n",
      "Telecom Data science\n",
      "Telecom Data analytics\n",
      "Telecom Digital transformation\n",
      "Telecom Machine learning\n",
      "Telecom Quantum computing\n",
      "Telecom Big data\n",
      "Telecom Artificial intellignce\n",
      "Telecom Modelling\n",
      "Telecom Internet of things\n",
      "Telecom Distributed ledger\n",
      "Telecom Blockchain\n",
      "Telecom Natural Language Processing\n",
      "Telecom Python\n",
      "Telecom R\n",
      "Telecom Java\n",
      "Telecom Matlab\n",
      "Telecom Telecom network\n",
      "Amazon Data science\n",
      "Amazon Data analytics\n",
      "Amazon Digital transformation\n",
      "Amazon Machine learning\n",
      "Amazon Quantum computing\n",
      "Amazon Big data\n",
      "Amazon Artificial intellignce\n",
      "Amazon Modelling\n",
      "Amazon Internet of things\n",
      "Amazon Distributed ledger\n",
      "Amazon Blockchain\n",
      "Amazon Natural Language Processing\n",
      "Amazon Python\n",
      "Amazon R\n",
      "Amazon Java\n",
      "Amazon Matlab\n",
      "Amazon Telecom network\n",
      "Google Data science\n",
      "Google Data analytics\n",
      "Google Digital transformation\n",
      "Google Machine learning\n",
      "Google Quantum computing\n",
      "Google Big data\n",
      "Google Artificial intellignce\n",
      "Google Modelling\n",
      "Google Internet of things\n",
      "Google Distributed ledger\n",
      "Google Blockchain\n",
      "Google Natural Language Processing\n",
      "Google Python\n",
      "Google R\n",
      "Google Java\n",
      "Google Matlab\n",
      "Google Telecom network\n",
      "Facebook Data science\n",
      "Facebook Data analytics\n",
      "Facebook Digital transformation\n",
      "Facebook Machine learning\n",
      "Facebook Quantum computing\n",
      "Facebook Big data\n",
      "Facebook Artificial intellignce\n",
      "Facebook Modelling\n",
      "Facebook Internet of things\n",
      "Facebook Distributed ledger\n",
      "Facebook Blockchain\n",
      "Facebook Natural Language Processing\n",
      "Facebook Python\n",
      "Facebook R\n",
      "Facebook Java\n",
      "Facebook Matlab\n",
      "Facebook Telecom network\n",
      "Microsoft Data science\n",
      "Microsoft Data analytics\n",
      "Microsoft Digital transformation\n",
      "Microsoft Machine learning\n",
      "Microsoft Quantum computing\n",
      "Microsoft Big data\n",
      "Microsoft Artificial intellignce\n",
      "Microsoft Modelling\n",
      "Microsoft Internet of things\n",
      "Microsoft Distributed ledger\n",
      "Microsoft Blockchain\n",
      "Microsoft Natural Language Processing\n",
      "Microsoft Python\n",
      "Microsoft R\n",
      "Microsoft Java\n",
      "Microsoft Matlab\n",
      "Microsoft Telecom network\n"
     ]
    }
   ],
   "source": [
    "# Number of tweets to get for each search\n",
    "N = 1000\n",
    "rows = {}\n",
    "\n",
    "i = 0\n",
    "for w in words_to_search:\n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(w)\\\n",
    "                                               .setSince(\"2010-05-06\")\\\n",
    "                                               .setUntil(\"2020-05-06\")\\\n",
    "                                               .setMaxTweets(N)\n",
    "    \n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    print(w)\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        rows[i] = {'id':tweet.id, 'permalink':tweet.permalink, 'username':tweet.username, 'to':tweet.to, \n",
    "                   'text':tweet.text, 'date':tweet.date, 'retweets':tweet.retweets, 'favorites':tweet.favorites,\n",
    "                   'mentions':tweet.mentions, 'hashtags':tweet.hashtags, 'geo':tweet.geo}\n",
    "        i += 1\n",
    "\n",
    "df_twitter = pd.DataFrame.from_dict(rows, \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T07:36:27.756184Z",
     "start_time": "2020-05-07T07:36:27.673188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>username</th>\n",
       "      <th>to</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1252157927869296646</td>\n",
       "      <td>https://twitter.com/EngineersDay/status/125215...</td>\n",
       "      <td>EngineersDay</td>\n",
       "      <td>None</td>\n",
       "      <td>[Job] 2020 Vodafone Portugal Youth Internship ...</td>\n",
       "      <td>2020-04-20 08:51:28+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>#Lisboa #Mining #Engineering #Jobs</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1255535008577708032</td>\n",
       "      <td>https://twitter.com/MSFTTelco/status/125553500...</td>\n",
       "      <td>MSFTTelco</td>\n",
       "      <td>TelecomTV</td>\n",
       "      <td>Join #telecom experts from @DeutscheTelekom, @...</td>\n",
       "      <td>2020-04-29 16:30:47+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>@deutschetelekom @Telefonica @vodafone @ericss...</td>\n",
       "      <td>#telecom #edge #AI #5G</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1243560912562401280</td>\n",
       "      <td>https://twitter.com/vf_institute/status/124356...</td>\n",
       "      <td>vf_institute</td>\n",
       "      <td>None</td>\n",
       "      <td>“When we see machine learning, let’s make it c...</td>\n",
       "      <td>2020-03-27 15:30:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>@audreyt @audreyt @agoerlach @vodafone_medien ...</td>\n",
       "      <td>#vTalk #AIandI</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1210879311122771968</td>\n",
       "      <td>https://twitter.com/Linney1851/status/12108793...</td>\n",
       "      <td>Linney1851</td>\n",
       "      <td>None</td>\n",
       "      <td>Here's a little taster of the tech we can look...</td>\n",
       "      <td>2019-12-28 11:04:59+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>#bbcnews #tech #spacetravel #google #vodafone</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1257637711709523968</td>\n",
       "      <td>https://twitter.com/Analyticsindiam/status/125...</td>\n",
       "      <td>Analyticsindiam</td>\n",
       "      <td>None</td>\n",
       "      <td>Dr. Sanjeev Chaube is currently the Head of Bi...</td>\n",
       "      <td>2020-05-05 11:46:10+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                          permalink  \\\n",
       "1  1252157927869296646  https://twitter.com/EngineersDay/status/125215...   \n",
       "2  1255535008577708032  https://twitter.com/MSFTTelco/status/125553500...   \n",
       "3  1243560912562401280  https://twitter.com/vf_institute/status/124356...   \n",
       "4  1210879311122771968  https://twitter.com/Linney1851/status/12108793...   \n",
       "6  1257637711709523968  https://twitter.com/Analyticsindiam/status/125...   \n",
       "\n",
       "          username         to  \\\n",
       "1     EngineersDay       None   \n",
       "2        MSFTTelco  TelecomTV   \n",
       "3     vf_institute       None   \n",
       "4       Linney1851       None   \n",
       "6  Analyticsindiam       None   \n",
       "\n",
       "                                                text  \\\n",
       "1  [Job] 2020 Vodafone Portugal Youth Internship ...   \n",
       "2  Join #telecom experts from @DeutscheTelekom, @...   \n",
       "3  “When we see machine learning, let’s make it c...   \n",
       "4  Here's a little taster of the tech we can look...   \n",
       "6  Dr. Sanjeev Chaube is currently the Head of Bi...   \n",
       "\n",
       "                        date retweets favorites  \\\n",
       "1  2020-04-20 08:51:28+00:00        0         0   \n",
       "2  2020-04-29 16:30:47+00:00        2         4   \n",
       "3  2020-03-27 15:30:00+00:00        2        17   \n",
       "4  2019-12-28 11:04:59+00:00        1         0   \n",
       "6  2020-05-05 11:46:10+00:00        1         0   \n",
       "\n",
       "                                            mentions  \\\n",
       "1                                                      \n",
       "2  @deutschetelekom @Telefonica @vodafone @ericss...   \n",
       "3  @audreyt @audreyt @agoerlach @vodafone_medien ...   \n",
       "4                                                      \n",
       "6                                                      \n",
       "\n",
       "                                        hashtags geo  \n",
       "1             #Lisboa #Mining #Engineering #Jobs      \n",
       "2                         #telecom #edge #AI #5G      \n",
       "3                                 #vTalk #AIandI      \n",
       "4  #bbcnews #tech #spacetravel #google #vodafone      \n",
       "6                                                     "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the first tweet's date\n",
    "min(df_twitter['date'])\n",
    "\n",
    "# Dropping duplicated tweets\n",
    "df_twitter.drop_duplicates(keep=False, inplace=True)\n",
    "\n",
    "# Printing the dataframe's head \n",
    "df_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving tweets to csv file\n",
    "df_twitter.to_csv('Twitter_mine.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Scraper (Kseniya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By accessing Pushshift API through building an URL with relevant parameters (query \n",
    "#with  key words, start date and end date) scrape posts from all subreddits\n",
    "\n",
    "def getPushshiftData(query, after, before):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?q='+query+'&size=1000&after='+after+'&before='+before+'&filter=title,selftext,created_utc&sort=asc'\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect relevant information (title,post text and date) from json retreived by  \n",
    "#getPushshiftData() function into a list\n",
    "def collectData(subm):\n",
    "    global posts\n",
    "    \n",
    "    subData = list() #list to store data points\n",
    "    \n",
    "    title = subm['title'] \n",
    "    #some posts just have a title without post's text and link to \n",
    "    #the article -> the object does not have 'selftext' field\n",
    "    if 'selftext' in subm:\n",
    "        postText = subm['selftext']\n",
    "    #if there is just a title\n",
    "    else:\n",
    "        postText = ''\n",
    "    createdDate = datetime.datetime.fromtimestamp(subm['created_utc']).strftime('%Y-%m-%d')\n",
    "    \n",
    "    posts.append([title,postText,createdDate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the parameters\n",
    "\n",
    "#start date 06/05/2010 - 1273104000 unix timestamp\n",
    "startDate = '1273104000'\n",
    "#end date 06/05/2020 -  1588723200 unix timestamp\n",
    "endDate = '1588723200'\n",
    "\n",
    "#list that will contain posts and then will be converted to dataframe\n",
    "posts = []\n",
    "\n",
    "#for the correct search each key word and subject should be framed by double quotes\n",
    "keywordlist = ['\"{0}\"'.format(w) for w in keywords]\n",
    "subject = ['\"{0}\"'.format(w) for w in subject]\n",
    "\n",
    "#query creation '|' means OR, '+' means AND \n",
    "#API will search for posts that contain at least one word from keyword list AND subject simultaneously\n",
    "keywordQuery = '('+'|'.join(keywordlist)+')'\n",
    "queries = [keywordQuery+'+'+comp for comp in subject]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through each query\n",
    "for query in queries:\n",
    "    #get maximum number of posts after the first call in order to get the last post's date \n",
    "    #that will be updated then in while loop\n",
    "    data = getPushshiftData(query,startDate,endDate)\n",
    "    # Will run until all posts have been gathered \n",
    "    # from the 'after' date up until 'before' date\n",
    "    while len(data) > 0:\n",
    "        #collect data from each API call\n",
    "        for submission in data:\n",
    "            collectData(submission)\n",
    "        # Calls getPushshiftData() with the created date of the last submission and update the 'after' date\n",
    "        after = str(data[-1]['created_utc'])\n",
    "        data = getPushshiftData(query, after, endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe with reddit posts\n",
    "posts = pd.DataFrame(posts,columns=['title', 'body', 'created'])\n",
    "\n",
    "# Dropping duplicated posts\n",
    "posts.drop_duplicates(keep=False, inplace=True)\n",
    "\n",
    "# Printing the dataframe's head \n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T12:39:02.907163Z",
     "start_time": "2020-05-07T12:39:01.702170Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'posts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5fa04030d2da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Saving the Economist data to csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mposts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Reddit_mine.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'posts' is not defined"
     ]
    }
   ],
   "source": [
    "# Saving the Reddit's data to csv \n",
    "posts.to_csv('Reddit_mine.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Press Release Data (Chiron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T19:46:03.281666Z",
     "start_time": "2020-05-06T19:43:07.943026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading csv file in chunks\n",
    "temp_list = []\n",
    "chunksize = 10 ** 5\n",
    "\n",
    "for chunk in pd.read_csv(r'C:\\Users\\Chiron\\Desktop\\ARP\\all-the-news-2-1\\all-the-news-2-1.csv', \n",
    "                         chunksize=chunksize, low_memory=False):\n",
    "    temp_list.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T19:46:05.794627Z",
     "start_time": "2020-05-06T19:46:05.783623Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe with all the news data\n",
    "frames = []\n",
    "for df in temp_list:\n",
    "    frames.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T19:55:00.886413Z",
     "start_time": "2020-05-06T19:46:07.468189Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe with all the news data\n",
    "news_df = pd.concat(temp_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:12:47.461719Z",
     "start_time": "2020-05-06T19:55:03.341595Z"
    }
   },
   "outputs": [],
   "source": [
    "# Deleting  irrelevant columns\n",
    "news_df.drop(news_df.columns[[0, 1]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:22:48.012541Z",
     "start_time": "2020-05-06T20:12:51.324579Z"
    }
   },
   "outputs": [],
   "source": [
    "# Deleting all articles which contains NAs\n",
    "news_df = news_df[news_df['article'].notna()]\n",
    "\n",
    "# Printing the dataframe's head\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T07:32:23.130194Z",
     "start_time": "2020-05-06T07:32:22.567601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Showing the count for each publisher\n",
    "publishers_count = news_df.groupby('publication')['publication'].count()\n",
    "publishers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:22:57.766788Z",
     "start_time": "2020-05-06T20:22:54.059126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>url</th>\n",
       "      <th>section</th>\n",
       "      <th>publication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>640996</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Economic milestones of the year ahead - Daily ...</td>\n",
       "      <td>A NUMBER of economic trends that have been sim...</td>\n",
       "      <td>https://www.economist.com/graphic-detail/2016/...</td>\n",
       "      <td>graphic-detail</td>\n",
       "      <td>Economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640997</th>\n",
       "      <td>2016-01-02 00:00:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A strange sort of welcome - Travel visas</td>\n",
       "      <td>THE rise of big emerging economies like China ...</td>\n",
       "      <td>https://www.economist.com/business/2016/01/02/...</td>\n",
       "      <td>business</td>\n",
       "      <td>Economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640999</th>\n",
       "      <td>2016-01-02 00:00:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Social saints, fiscal fiends - Schumpeter</td>\n",
       "      <td>PFIZER has always prided itself on its commitm...</td>\n",
       "      <td>https://www.economist.com/business/2016/01/02/...</td>\n",
       "      <td>business</td>\n",
       "      <td>Economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641006</th>\n",
       "      <td>2016-01-02 00:00:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What if they were really set free? - Ethiopia</td>\n",
       "      <td>THE Ben Abeba restaurant is a spiral-shaped co...</td>\n",
       "      <td>https://www.economist.com/middle-east-and-afri...</td>\n",
       "      <td>middle-east-and-africa</td>\n",
       "      <td>Economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641008</th>\n",
       "      <td>2016-01-02 00:00:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Exit, pursued by bear - Free exchange</td>\n",
       "      <td>IT IS more than two weeks since the Federal Re...</td>\n",
       "      <td>https://www.economist.com/finance-and-economic...</td>\n",
       "      <td>finance-and-economics</td>\n",
       "      <td>Economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568493</th>\n",
       "      <td>2020-02-20 00:00:00</td>\n",
       "      <td>2020</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Economics - Are data more like oil or sunlight...</td>\n",
       "      <td>Champions of the “open-data” movement push org...</td>\n",
       "      <td>https://www.economist.com/special-report/2020/...</td>\n",
       "      <td>special-report</td>\n",
       "      <td>Economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598763</th>\n",
       "      <td>2020-02-27 00:00:00</td>\n",
       "      <td>2020</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The bedchamber and the axe - Thomas Cromwell, ...</td>\n",
       "      <td>There was a former age, it seems, when wives w...</td>\n",
       "      <td>https://www.economist.com/books-and-arts/2020/...</td>\n",
       "      <td>books-and-arts</td>\n",
       "      <td>Economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598770</th>\n",
       "      <td>2020-02-27 00:00:00</td>\n",
       "      <td>2020</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Immortal words - The tragic genius of Abraham ...</td>\n",
       "      <td>With malice toward none, with charity to all; ...</td>\n",
       "      <td>https://www.economist.com/books-and-arts/2020/...</td>\n",
       "      <td>books-and-arts</td>\n",
       "      <td>Economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635055</th>\n",
       "      <td>2020-03-12 00:00:00</td>\n",
       "      <td>2020</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The human genome project - Genomics took a lon...</td>\n",
       "      <td>The first genome cost, by some estimates, $3bn...</td>\n",
       "      <td>https://www.economist.com/technology-quarterly...</td>\n",
       "      <td>technology-quarterly</td>\n",
       "      <td>Economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635097</th>\n",
       "      <td>2020-03-12 00:00:00</td>\n",
       "      <td>2020</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The pharmaceutical industry - New drugs are co...</td>\n",
       "      <td>Zolgensma is the most expensive drug ever brou...</td>\n",
       "      <td>https://www.economist.com/technology-quarterly...</td>\n",
       "      <td>technology-quarterly</td>\n",
       "      <td>Economist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23200 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  year  month day author  \\\n",
       "640996   2016-01-01 00:00:00  2016    1.0   1    NaN   \n",
       "640997   2016-01-02 00:00:00  2016    1.0   2    NaN   \n",
       "640999   2016-01-02 00:00:00  2016    1.0   2    NaN   \n",
       "641006   2016-01-02 00:00:00  2016    1.0   2    NaN   \n",
       "641008   2016-01-02 00:00:00  2016    1.0   2    NaN   \n",
       "...                      ...   ...    ...  ..    ...   \n",
       "2568493  2020-02-20 00:00:00  2020    2.0  20    NaN   \n",
       "2598763  2020-02-27 00:00:00  2020    2.0  27    NaN   \n",
       "2598770  2020-02-27 00:00:00  2020    2.0  27    NaN   \n",
       "2635055  2020-03-12 00:00:00  2020    3.0  12    NaN   \n",
       "2635097  2020-03-12 00:00:00  2020    3.0  12    NaN   \n",
       "\n",
       "                                                     title  \\\n",
       "640996   Economic milestones of the year ahead - Daily ...   \n",
       "640997            A strange sort of welcome - Travel visas   \n",
       "640999           Social saints, fiscal fiends - Schumpeter   \n",
       "641006       What if they were really set free? - Ethiopia   \n",
       "641008               Exit, pursued by bear - Free exchange   \n",
       "...                                                    ...   \n",
       "2568493  Economics - Are data more like oil or sunlight...   \n",
       "2598763  The bedchamber and the axe - Thomas Cromwell, ...   \n",
       "2598770  Immortal words - The tragic genius of Abraham ...   \n",
       "2635055  The human genome project - Genomics took a lon...   \n",
       "2635097  The pharmaceutical industry - New drugs are co...   \n",
       "\n",
       "                                                   article  \\\n",
       "640996   A NUMBER of economic trends that have been sim...   \n",
       "640997   THE rise of big emerging economies like China ...   \n",
       "640999   PFIZER has always prided itself on its commitm...   \n",
       "641006   THE Ben Abeba restaurant is a spiral-shaped co...   \n",
       "641008   IT IS more than two weeks since the Federal Re...   \n",
       "...                                                    ...   \n",
       "2568493  Champions of the “open-data” movement push org...   \n",
       "2598763  There was a former age, it seems, when wives w...   \n",
       "2598770  With malice toward none, with charity to all; ...   \n",
       "2635055  The first genome cost, by some estimates, $3bn...   \n",
       "2635097  Zolgensma is the most expensive drug ever brou...   \n",
       "\n",
       "                                                       url  \\\n",
       "640996   https://www.economist.com/graphic-detail/2016/...   \n",
       "640997   https://www.economist.com/business/2016/01/02/...   \n",
       "640999   https://www.economist.com/business/2016/01/02/...   \n",
       "641006   https://www.economist.com/middle-east-and-afri...   \n",
       "641008   https://www.economist.com/finance-and-economic...   \n",
       "...                                                    ...   \n",
       "2568493  https://www.economist.com/special-report/2020/...   \n",
       "2598763  https://www.economist.com/books-and-arts/2020/...   \n",
       "2598770  https://www.economist.com/books-and-arts/2020/...   \n",
       "2635055  https://www.economist.com/technology-quarterly...   \n",
       "2635097  https://www.economist.com/technology-quarterly...   \n",
       "\n",
       "                        section publication  \n",
       "640996           graphic-detail   Economist  \n",
       "640997                 business   Economist  \n",
       "640999                 business   Economist  \n",
       "641006   middle-east-and-africa   Economist  \n",
       "641008    finance-and-economics   Economist  \n",
       "...                         ...         ...  \n",
       "2568493          special-report   Economist  \n",
       "2598763          books-and-arts   Economist  \n",
       "2598770          books-and-arts   Economist  \n",
       "2635055    technology-quarterly   Economist  \n",
       "2635097    technology-quarterly   Economist  \n",
       "\n",
       "[23200 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting articles from the Economist\n",
    "economist = news_df.loc[news_df['publication'] == 'Economist']\n",
    "economist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T12:41:48.843970Z",
     "start_time": "2020-05-07T12:41:48.812969Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'economist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1fac325e90eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Selecting only the relevant articles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0meconomist_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meconomist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'article'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'|'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_to_search\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0meconomist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meconomist_final\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'economist' is not defined"
     ]
    }
   ],
   "source": [
    "# Selecting only the relevant articles\n",
    "economist_final = economist['article'].str.contains('|'.join(words_to_search), regex=True)\n",
    "economist[economist_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T20:33:31.691960Z",
     "start_time": "2020-05-06T20:33:31.659955Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the Economist data to csv \n",
    "economist[economist_final].to_csv('Economist.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.444px",
    "left": "1175.77px",
    "right": "20px",
    "top": "29.9896px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
